{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    An LSTM autoencoder model for time series\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_latent,\n",
    "        time_window,\n",
    "        n_features=1,\n",
    "        network_shape=[],\n",
    "        latent_regularizer=None,\n",
    "        rnn_opts=dict(),\n",
    "        activation_func=tf.keras.layers.ELU(alpha=1.0),\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.time_window = time_window\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Initialize state\n",
    "        tf.random.set_seed(random_state)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential()\n",
    "        self.encoder.add(tf.keras.layers.InputLayer(input_shape=(time_window, n_features)))\n",
    "        self.encoder.add(tf.keras.layers.GaussianNoise(0.5)) # smooths the output\n",
    "        \n",
    "        for i, hidden_size in enumerate(network_shape):\n",
    "            self.encoder.add(\n",
    "                tf.keras.layers.LSTM(\n",
    "                    hidden_size,\n",
    "                    #input_shape=(time_window, n_features),\n",
    "                    return_sequences=True,\n",
    "                    name=\"lstm_encoder_\"+str(i),\n",
    "                    **rnn_opts\n",
    "                )\n",
    "            )\n",
    "            self.encoder.add(tf.keras.layers.BatchNormalization())\n",
    "            self.encoder.add(tf.keras.layers.Activation(activation_func))\n",
    "        self.encoder.add(\n",
    "            tf.keras.layers.LSTM(\n",
    "                n_latent,\n",
    "                #input_shape=(time_window, n_features),\n",
    "                return_sequences=False,\n",
    "                name=\"lstm_encoder_final\",\n",
    "                **rnn_opts\n",
    "            )\n",
    "        )\n",
    "        self.encoder.add(tf.keras.layers.BatchNormalization(activity_regularizer=latent_regularizer))\n",
    "            \n",
    "        ## Decoder\n",
    "        self.decoder = tf.keras.Sequential()\n",
    "        self.decoder.add(tf.keras.layers.GaussianNoise(0.5, input_shape=(n_latent,)))\n",
    "        self.decoder.add(tf.keras.layers.RepeatVector(time_window))\n",
    "        self.decoder.add(\n",
    "            tf.keras.layers.LSTM(\n",
    "                n_latent,\n",
    "                #input_shape=(time_window, n_features),\n",
    "                return_sequences=True,\n",
    "                name=\"lstm_decoder_initial\",\n",
    "                **rnn_opts\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        for i, hidden_size in enumerate(network_shape[::-1]):\n",
    "            self.decoder.add(\n",
    "                tf.keras.layers.LSTM(\n",
    "                    hidden_size, \n",
    "                    return_sequences=True, \n",
    "                    go_backwards=True, \n",
    "                    name=\"lstm_decoder_\"+str(i),\n",
    "                    **rnn_opts\n",
    "                )\n",
    "            )\n",
    "        self.decoder.add(\n",
    "            tf.keras.layers.LSTM(\n",
    "                n_features, \n",
    "                return_sequences=True, \n",
    "                go_backwards=True, \n",
    "                name=\"lstm_decoder_final\"),\n",
    "                **rnn_opts\n",
    "        )\n",
    "        self.decoder.add(tf.keras.layers.BatchNormalization())\n",
    "        #self.decoder.add(tf.keras.layers.Activation(activation_func))\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.decoder(self.encoder(inputs))\n",
    "        return outputs\n",
    "this is called in the below code\n",
    "\n",
    "class TimeSeriesEmbedding:\n",
    "    \"\"\"Base class for time series embedding\n",
    "    \n",
    "    Properties\n",
    "    ----------\n",
    "    \n",
    "    train_history : dict\n",
    "        The training history of the model\n",
    "    \n",
    "    model : \"lstm\" | \"mlp\" | \"tica\" | \"etd\" | \"delay\"\n",
    "        The type of model to use for the embedding.\n",
    "        \n",
    "    n_latent : int\n",
    "        The embedding dimension\n",
    "        \n",
    "    n_features : int\n",
    "        The number of channels in the time series\n",
    "    \n",
    "    **kwargs : dict\n",
    "        Keyword arguments passed to the model\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_latent,\n",
    "        time_window=10, \n",
    "        n_features=1, \n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.n_latent = n_latent\n",
    "        self.time_window = time_window\n",
    "        self.n_features = n_features\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        raise AttributeError(\"Derived class does not contain method.\")\n",
    "           \n",
    "    def transform(self, X, y=None):\n",
    "        raise AttributeError(\"Derived class does not contain method.\")\n",
    "\n",
    "    def fit_transform(self, X, y=None, **kwargs):\n",
    "        \"\"\"Fit the model with a time series X, and then embed X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_timepoints, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "        y : None\n",
    "            Ignored variable.\n",
    "            \n",
    "        kwargs : keyword arguments passed to the model's fit() method\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_timepoints, n_components)\n",
    "            Transformed values.\n",
    "        \"\"\"\n",
    "        self.fit(X, **kwargs)\n",
    "        return self.transform(X)\n",
    "class NeuralNetworkEmbedding(TimeSeriesEmbedding):\n",
    "    \"\"\"Base class autoencoder model for time series embedding\n",
    "    \n",
    "    Properties\n",
    "    ----------\n",
    "    \n",
    "    n_latent : int\n",
    "        The embedding dimension\n",
    "        \n",
    "    n_features : int\n",
    "        The number of channels in the time series\n",
    "    \n",
    "    **kwargs : dict\n",
    "        Keyword arguments passed to the model\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # # Default latent regularizer is FNN\n",
    "        # if np.isscalar(latent_regularizer):\n",
    "        #     latent_regularizer = FNN(latent_regularizer)\n",
    "    \n",
    "    def fit(\n",
    "        self, \n",
    "        X,\n",
    "        y=None,\n",
    "        subsample=None,\n",
    "        tau=0,\n",
    "        learning_rate=1e-3, \n",
    "        batch_size=100, \n",
    "        train_steps=200,\n",
    "        loss='mse',\n",
    "        verbose=0,\n",
    "        optimizer=\"adam\",\n",
    "        early_stopping=False\n",
    "    ):\n",
    "        \"\"\"Fit the model with a time series X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_timepoints, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "        y : None\n",
    "            Ignored variable.\n",
    "\n",
    "        subsample : int or None\n",
    "            If set to an integer, a random number of timepoints is selected\n",
    "            equal to that integer\n",
    "            \n",
    "        tau : int\n",
    "            The prediction time, or the number of timesteps to skip between \n",
    "            the input and output time series\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_timepoints, n_components)\n",
    "            Transformed values.\n",
    "        \"\"\"\n",
    "        # Make hankel matrix from dataset\n",
    "        Xs = standardize_ts(X)\n",
    "        \n",
    "        # X_train = hankel_matrix(Xs, self.time_window)\n",
    "        # Split the hankel matrix for a prediction task\n",
    "        X0 = hankel_matrix(Xs, self.time_window + tau)\n",
    "        X_train = X0[:, :self.time_window ]\n",
    "        Y_train = X0[:, -self.time_window:]\n",
    "        \n",
    "        \n",
    "        if subsample:\n",
    "            self.train_indices, _ = resample_dataset(\n",
    "                X_train, subsample, random_state=self.random_state\n",
    "            )\n",
    "            X_train = X_train[self.train_indices]\n",
    "            Y_train = Y_train[self.train_indices]\n",
    "\n",
    "\n",
    "        optimizers = {\n",
    "            \"adam\": tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            \"nadam\": tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "            # \"radam\": tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n",
    "        }\n",
    "\n",
    "        tf.random.set_seed(self.random_state)\n",
    "        np.random.seed(self.random_state)\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers[optimizer], \n",
    "            loss=loss,\n",
    "            #experimental_run_tf_function=False\n",
    "        )    \n",
    "        \n",
    "        if early_stopping:\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', patience=3)]\n",
    "        else:\n",
    "            callbacks = [None]\n",
    "        \n",
    "        self.train_history = self.model.fit(\n",
    "            x=tf.convert_to_tensor(X_train),                         \n",
    "            y=tf.convert_to_tensor(Y_train),\n",
    "            epochs=train_steps,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose\n",
    "        )\n",
    "             \n",
    "    def transform(self, X, y=None):\n",
    "        X_test = hankel_matrix(standardize_ts(X), self.time_window)\n",
    "        X_new = self.model.encoder.predict(X_test)\n",
    "        return X_new \n",
    "\n",
    "class LSTMEmbedding(NeuralNetworkEmbedding): \n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        kwargs.pop(\"time_window\")\n",
    "        if use_legacy:\n",
    "            self.model = LSTMAutoencoderLegacy(\n",
    "                self.n_latent,\n",
    "                self.time_window,\n",
    "                **kwargs\n",
    "            ) \n",
    "        else:\n",
    "            self.model = LSTMAutoencoder(\n",
    "                self.n_latent,\n",
    "                self.time_window,\n",
    "                **kwargs\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
